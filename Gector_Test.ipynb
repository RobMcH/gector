{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gector_Test.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vrbazFpDNvR4",
        "mXfjaDNlXQW9",
        "NzBMROYdbQ1E"
      ],
      "machine_shape": "hm",
      "mount_file_id": "1yD7-2-uM1B0t7TXsdla-tPfSgy1w5GOH",
      "authorship_tag": "ABX9TyPlnW0nSC33O+QWLRE+lNsc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobMcH/gector/blob/nils/Gector_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI2I9ZuHjbMW"
      },
      "source": [
        "Find out GPU type (The Colab-runtime must include a GPU - needs to be manually changed in the settings if not existent):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaqQz0nti1Kz",
        "outputId": "16a0d7d9-dbcc-464d-c8b9-e0ea59dda0b3"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-192e30b8-b833-74b6-d596-f8ea65ae886e)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbwpkpRr6tnI"
      },
      "source": [
        "# Setup Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrerIsIcJXYR"
      },
      "source": [
        "## Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVX70ZJeHqoo"
      },
      "source": [
        "Change working directory to gector-master copy in Google Drive \n",
        "\n",
        "*First connect to Google Drive via: Files (Click on File-Symbol in left-sidebar) -> Connect to Google Drive (Button at the top of the newly opened sidebar)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR2aLImpIW75",
        "outputId": "0068673d-e936-4631-cffe-699e80880e8b"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/gector-master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/gector-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8LamvNFJc0B"
      },
      "source": [
        "## Install requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE0MuD2sJyjg"
      },
      "source": [
        "Install all requirements as specified in the Gector requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhhI2IG_JqPA"
      },
      "source": [
        "pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w72hVakuV91k"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnXjipoLWC_U"
      },
      "source": [
        "Import the libraries that are needed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPjuoc9XWG8t"
      },
      "source": [
        "import nltk\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRbSIlWqVt-E"
      },
      "source": [
        "We have to download the NLTK punkt corpus since it is used later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm-Gy6ogVybk",
        "outputId": "8e01b9d4-49f3-458c-9e5b-f44e15688e0c"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSD4XQrrNS2_"
      },
      "source": [
        "# Pre-process data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrbazFpDNvR4"
      },
      "source": [
        "## Convert FCE from XML to parallel sentence format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMh5dW_RXIM0"
      },
      "source": [
        "The following datasets can be used:\n",
        "\n",
        "* All the public GEC datasets used in the paper can be downloaded from [here](https://www.cl.cam.ac.uk/research/nl/bea2019st/#data).\n",
        "* Synthetically created datasets can be generated/downloaded [here](https://github.com/awasthiabhijeet/PIE/tree/master/errorify).\n",
        "\n",
        "To test if everything works, we use the FCE v2.1 dataset.\n",
        "\n",
        "The GECToR repository already contains a script which expects the parent directory of the FCE dataset (which can be downloaded [here](https://ilexir.co.uk/datasets/index.html)).\n",
        "This conversion from the xml to the \"parallel sentences format\" which GECToR uses has to be done once.\n",
        "We created a new folder \"fce_output_folder\" which contains the processed results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSeRfSyZTZwf"
      },
      "source": [
        "#!python utils/prepare_clc_fce_data.py 'fce-released-dataset' --output 'fce_output_folder'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfC3UyCCWvFU"
      },
      "source": [
        "After this operation, our 'fce_output_folder' contains two txt files:\n",
        "- 'fce-original.txt': Each line contains the original sentence with a grammatical error\n",
        "- 'fce-applied.txt': Each line contains the corrected sentence (same order as in the origial file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXfjaDNlXQW9"
      },
      "source": [
        "## Convert parallel sentence format to GECToR specific format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYoJQ2bZXdKM"
      },
      "source": [
        "To train the model. the data needs to be preprocessed and converted to special format with the following command where:\n",
        "- s: Path to the source file (Original sentences w/ mistakes)\n",
        "- t: Path to the target file (Correct sentences w/o mistakes)\n",
        "- o: Path to the output file (the training data will be stored in this file)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqo51LJrXi4U",
        "outputId": "8042c237-0e63-4937-b07d-3b5a8d3dabd5"
      },
      "source": [
        "#!python utils/preprocess_data.py -s 'fce_output_folder/fce-original.txt' -t 'fce_output_folder/fce-applied.txt' -o 'fce_output_folder/training_data.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of raw dataset is 34490\n",
            "34490it [00:05, 5795.45it/s]\n",
            "Overall extracted 34490. Original TP 21525. Original TN 12965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzBMROYdbQ1E"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK8Yy4GLbUU9"
      },
      "source": [
        "To train the model we have to download a pretrained model from [here](https://github.com/grammarly/gector) and place the file in the 'pre-trained-models' folder. Then, we can run the following command:\n",
        "\n",
        "- --train_set: training data (txt) as generated before\n",
        "- --dev_set: validation data (txt) as generated before\n",
        "- --model_dir: directory of the pre-trained model\n",
        "- --batch_size: default batch-size is 32\n",
        "- --n_epoch: number of epochs for the training\n",
        "- --patience: Early stopping rounds (default = 3)\n",
        "- --lr: Learning rate\n",
        "- --predictor_dropout: Dropout rate for predictor (default = 0.0)\n",
        "- --transformer_model: Name of the transformer (choices=['bert', 'distilbert', 'gpt2', 'roberta', 'transformerxl', 'xlnet', 'albert'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDbtAsNTdLtl",
        "outputId": "5fb75b39-1d87-4fde-c140-a868be6bd02e"
      },
      "source": [
        "!python train.py --train_set 'fce_output_folder/training_data.txt' --dev_set 'fce_output_folder/training_data.txt' --model_dir 'pre-trained-models' --n_epoch 1 --transformer_model 'bert'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.4) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n",
            "2021-04-23 02:23:13.726731: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 1.07MB/s]\n",
            "21524it [00:02, 7445.27it/s]\n",
            "WARNING:root:vocabulary serialization directory pre-trained-models/vocabulary is not empty\n",
            "Data is loaded\n",
            "Downloading: 100% 433/433 [00:00<00:00, 399kB/s]\n",
            "Downloading: 100% 436M/436M [00:10<00:00, 41.6MB/s]\n",
            "Model is set\n",
            "Start training\n",
            "accuracy: 0.8604, loss: 1.9602 ||: : 673it [00:37, 18.17it/s]\n",
            "accuracy: 0.8671, loss: 1.3849 ||: : 673it [00:33, 20.06it/s]\n",
            "Model is dumped\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlvUe26g8m0V"
      },
      "source": [
        "# Make predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1s3pZiN8tsN"
      },
      "source": [
        "In order to make predictions we can run the following command:\n",
        "\n",
        "- --model_path: Path to the pre-trained model file\n",
        "- --vocab_path: Path to the model vocab file (default:'data/output_vocabulary')\n",
        "- --input_file: Path to the file for which we we want to know the model output\n",
        "- --output_file: Path to the file where the prediction results will be stored\n",
        "- --lowercase_tokens: Whether to lowercase tokens (default=0)\n",
        "- --transformer_model: ['bert', 'gpt2', 'transformerxl', 'xlnet', 'distilbert', 'roberta', 'albert']\n",
        "- --iteration_count: number of iterations we want to make during prediction\n",
        "- --additional_confidence: How many probability to add to $KEEP token (default=0)\n",
        "- --min_error_probability: As described in the paper\n",
        "- --special_tokens_fix: Use 0 for reproducability (default=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Sy5kQZ8sjf"
      },
      "source": [
        "!python predict.py --model_path 'pre-trained-models/bert_0_gector.th' --input_file 'fce_output_folder/fce-original.txt' \\\n",
        "                   --output_file 'fce_output_folder/fce_prediction.txt' --lowercase_tokens 0 --transformer_model 'bert' --iteration_count 5 --special_tokens_fix 0 \\\n",
        "                   --additional_confidence 0.1 --min_error_probability 0.41"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJCZq6I9LdHd"
      },
      "source": [
        "Our file: 'fce_prediction.txt' now contains the prediction for every sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0h3jBa6LoWO"
      },
      "source": [
        "# Evaluate predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3FjfAOpPP9W"
      },
      "source": [
        "To evaluate the predictions we need M2 files (since the M2 scorer only takes this format). Therefore, we have to install errant and its dependencies):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNtiywu9PPTO"
      },
      "source": [
        "!pip3 install -U pip setuptools wheel\n",
        "!pip3 install errant\n",
        "!python3 -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCVtUfFbRoLF"
      },
      "source": [
        "We can create the m2 files with the following command where:\n",
        "- --orig: Path to the file with out original sentences (w/ mistakes)\n",
        "- --cor: Path to the corrected sentences (w/o mistakes)\n",
        "- --out: Path where our m2-file will be created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nhzy341OQ3Zu",
        "outputId": "74a8de03-5875-4d81-dbb2-6dc14292f1c7"
      },
      "source": [
        "!errant_parallel -orig 'fce_output_folder/fce-original.txt' -cor 'fce_output_folder/fce-applied.txt' -out 'fce_output_folder/fce-gold.m2'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading resources...\n",
            "Processing parallel files...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQCHgXTYLst1"
      },
      "source": [
        "For the metric-evaluation, we use the M^2 scorer, from [this](https://github.com/nusnlp/m2scorer) repository. We copied the whole repository into the gector-master folder which now contains a 'm2scorer' folder. \n",
        "\n",
        "**Important: since the m2scorer is written in python2 some modifications have been made**\n",
        "\n",
        "We can than call the m2scorer.py script with the following arguments:\n",
        "- First argument: Path to the file with the prediction (txt - one sentence per line)\n",
        "- Second argument: File with the corrections in m2 format (as created before)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sej9T4luL7FO",
        "outputId": "4acf591a-a262-4631-cda6-6c103b749184"
      },
      "source": [
        "!python m2scorer/scripts/m2scorer.py 'fce_output_folder/fce_prediction.txt' 'fce_output_folder/fce-gold.m2'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision   : 0.6208\n",
            "Recall      : 0.4167\n",
            "F_0.5       : 0.5654\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}