{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gector_Test.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bbwpkpRr6tnI",
        "YrerIsIcJXYR",
        "w8LamvNFJc0B",
        "w72hVakuV91k",
        "BSD4XQrrNS2_",
        "vrbazFpDNvR4",
        "mXfjaDNlXQW9",
        "NzBMROYdbQ1E"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1yD7-2-uM1B0t7TXsdla-tPfSgy1w5GOH",
      "authorship_tag": "ABX9TyNP74ytjjJmPpOpijmXEjcr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobMcH/gector/blob/nils/Gector_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI2I9ZuHjbMW"
      },
      "source": [
        "Find out GPU type (The Colab-runtime must include a GPU - needs to be manually changed in the settings if not existent):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaqQz0nti1Kz",
        "outputId": "01a9ac79-103d-4b28-ef83-cc7ef5a91553"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-080c496e-e1c6-17a6-faa6-3d2239fe3c6d)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbwpkpRr6tnI"
      },
      "source": [
        "# Setup Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrerIsIcJXYR"
      },
      "source": [
        "## Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVX70ZJeHqoo"
      },
      "source": [
        "Change working directory to gector-master copy in Google Drive \n",
        "\n",
        "*First connect to Google Drive via: Files (Click on File-Symbol in left-sidebar) -> Connect to Google Drive (Button at the top of the newly opened sidebar)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR2aLImpIW75",
        "outputId": "a35df810-4634-4a61-9838-d166bc7c27ac"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/gector-master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/gector-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8LamvNFJc0B"
      },
      "source": [
        "## Install requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE0MuD2sJyjg"
      },
      "source": [
        "Install all requirements as specified in the Gector requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhhI2IG_JqPA"
      },
      "source": [
        "pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w72hVakuV91k"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnXjipoLWC_U"
      },
      "source": [
        "Import the libraries that are needed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPjuoc9XWG8t"
      },
      "source": [
        "import nltk\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRbSIlWqVt-E"
      },
      "source": [
        "We have to download the NLTK punkt corpus since it is used later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm-Gy6ogVybk",
        "outputId": "2ffbbaeb-87fb-45c7-ccff-77fc2dc55d82"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSD4XQrrNS2_"
      },
      "source": [
        "# Pre-process data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrbazFpDNvR4"
      },
      "source": [
        "## Convert FCE from XML to parallel sentence format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMh5dW_RXIM0"
      },
      "source": [
        "The following datasets can be used:\n",
        "\n",
        "* All the public GEC datasets used in the paper can be downloaded from [here](https://www.cl.cam.ac.uk/research/nl/bea2019st/#data).\n",
        "* Synthetically created datasets can be generated/downloaded [here](https://github.com/awasthiabhijeet/PIE/tree/master/errorify).\n",
        "\n",
        "To test if everything works, we use the FCE v2.1 dataset.\n",
        "\n",
        "The GECToR repository already contains a script which expects the parent directory of the FCE dataset (which can be downloaded [here](https://ilexir.co.uk/datasets/index.html)).\n",
        "This conversion from the xml to the \"parallel sentences format\" which GECToR uses has to be done once.\n",
        "We created a new folder \"fce_output_folder\" which contains the processed results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSeRfSyZTZwf"
      },
      "source": [
        "#!python utils/prepare_clc_fce_data.py 'fce-released-dataset' --output 'fce_output_folder'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfC3UyCCWvFU"
      },
      "source": [
        "After this operation, our 'fce_output_folder' contains two txt files:\n",
        "- 'fce-original.txt': Each line contains the original sentence with a grammatical error\n",
        "- 'fce-applied.txt': Each line contains the corrected sentence (same order as in the origial file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXfjaDNlXQW9"
      },
      "source": [
        "## Convert parallel sentence format to GECToR specific format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYoJQ2bZXdKM"
      },
      "source": [
        "To train the model. the data needs to be preprocessed and converted to special format with the following command where:\n",
        "- s: Path to the source file (Original sentences w/ mistakes)\n",
        "- t: Path to the target file (Correct sentences w/o mistakes)\n",
        "- o: Path to the output file (the training data will be stored in this file)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqo51LJrXi4U",
        "outputId": "8042c237-0e63-4937-b07d-3b5a8d3dabd5"
      },
      "source": [
        "#!python utils/preprocess_data.py -s 'fce_output_folder/fce-original.txt' -t 'fce_output_folder/fce-applied.txt' -o 'fce_output_folder/training_data.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of raw dataset is 34490\n",
            "34490it [00:05, 5795.45it/s]\n",
            "Overall extracted 34490. Original TP 21525. Original TN 12965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzBMROYdbQ1E"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK8Yy4GLbUU9"
      },
      "source": [
        "To train the model we have to download a pretrained model from [here](https://github.com/grammarly/gector) and place the file in the 'pre-trained-models' folder. Then, we can run the following command:\n",
        "\n",
        "- --train_set: training data (txt) as generated before\n",
        "- --dev_set: validation data (txt) as generated before\n",
        "- --model_dir: directory of the pre-trained model\n",
        "- --batch_size: default batch-size is 32\n",
        "- --n_epoch: number of epochs for the training\n",
        "- --patience: Early stopping rounds (default = 3)\n",
        "- --lr: Learning rate\n",
        "- --predictor_dropout: Dropout rate for predictor (default = 0.0)\n",
        "- --transformer_model: Name of the transformer (choices=['bert', 'distilbert', 'gpt2', 'roberta', 'transformerxl', 'xlnet', 'albert'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDbtAsNTdLtl",
        "outputId": "5fb75b39-1d87-4fde-c140-a868be6bd02e"
      },
      "source": [
        "!python train.py --train_set 'fce_output_folder/training_data.txt' --dev_set 'fce_output_folder/training_data.txt' --model_dir 'pre-trained-models' --n_epoch 1 --transformer_model 'bert'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.4) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n",
            "2021-04-23 02:23:13.726731: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 1.07MB/s]\n",
            "21524it [00:02, 7445.27it/s]\n",
            "WARNING:root:vocabulary serialization directory pre-trained-models/vocabulary is not empty\n",
            "Data is loaded\n",
            "Downloading: 100% 433/433 [00:00<00:00, 399kB/s]\n",
            "Downloading: 100% 436M/436M [00:10<00:00, 41.6MB/s]\n",
            "Model is set\n",
            "Start training\n",
            "accuracy: 0.8604, loss: 1.9602 ||: : 673it [00:37, 18.17it/s]\n",
            "accuracy: 0.8671, loss: 1.3849 ||: : 673it [00:33, 20.06it/s]\n",
            "Model is dumped\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}